The project covers a sign language translation system that is accurate and easy to use. Previous approaches to sign language translation have mainly focus on the use of various different machine learning models to achieve an accurate recognition of hand gesture of the alphabet, which can be a difficult due to some of the alphabet being similar to one another, like M and N, S and T, as well as some of the gesture are moving gesture, like J and Z. A combination of GLCM and CNN as the proposed method have been applied to a dataset size of 87,000 images of Alphabetic sign gesture. GLCM is a texture analysis method that extracts features from images, while CNN is a powerful machine learning technique that is well-suited for image classification tasks. The experimental results have shown that our GLCM-CNN method has successfully reached an accuracy of 88%. Based on our findings, the combination of these two methods allows us to build a model that is able to accurately recognize and classify sign language gestures.
